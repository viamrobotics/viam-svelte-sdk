// @generated by protoc-gen-es v1.10.0
// @generated from file app/mlinference/v1/ml_inference.proto (package viam.app.mlinference.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import type { BinaryReadOptions, FieldList, JsonReadOptions, JsonValue, PartialMessage, PlainMessage } from "@bufbuild/protobuf";
import { Message, proto3 } from "@bufbuild/protobuf";
import type { Annotations, BinaryID } from "../../data/v1/data_pb.js";
import type { FlatTensors } from "../../../service/mlmodel/v1/mlmodel_pb.js";

/**
 * @generated from message viam.app.mlinference.v1.GetInferenceRequest
 */
export declare class GetInferenceRequest extends Message<GetInferenceRequest> {
  /**
   * The model framework and model type are inferred from the ML model registry item;
   * For valid model types (classification, detections) we will return the formatted
   * labels or annotations from the associated inference outputs.
   *
   * @generated from field: string registry_item_id = 1;
   */
  registryItemId: string;

  /**
   * @generated from field: string registry_item_version = 2;
   */
  registryItemVersion: string;

  /**
   * @generated from field: viam.app.data.v1.BinaryID binary_id = 3;
   */
  binaryId?: BinaryID;

  /**
   * @generated from field: string organization_id = 4;
   */
  organizationId: string;

  constructor(data?: PartialMessage<GetInferenceRequest>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "viam.app.mlinference.v1.GetInferenceRequest";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): GetInferenceRequest;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): GetInferenceRequest;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): GetInferenceRequest;

  static equals(a: GetInferenceRequest | PlainMessage<GetInferenceRequest> | undefined, b: GetInferenceRequest | PlainMessage<GetInferenceRequest> | undefined): boolean;
}

/**
 * @generated from message viam.app.mlinference.v1.GetInferenceResponse
 */
export declare class GetInferenceResponse extends Message<GetInferenceResponse> {
  /**
   * @generated from field: viam.service.mlmodel.v1.FlatTensors output_tensors = 1;
   */
  outputTensors?: FlatTensors;

  /**
   * @generated from field: viam.app.data.v1.Annotations annotations = 2;
   */
  annotations?: Annotations;

  constructor(data?: PartialMessage<GetInferenceResponse>);

  static readonly runtime: typeof proto3;
  static readonly typeName = "viam.app.mlinference.v1.GetInferenceResponse";
  static readonly fields: FieldList;

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): GetInferenceResponse;

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): GetInferenceResponse;

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): GetInferenceResponse;

  static equals(a: GetInferenceResponse | PlainMessage<GetInferenceResponse> | undefined, b: GetInferenceResponse | PlainMessage<GetInferenceResponse> | undefined): boolean;
}

